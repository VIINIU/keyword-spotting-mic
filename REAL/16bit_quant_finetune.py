import numpy as np
import snntorch as snn
from snntorch import surrogate
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import os
import glob
import multiprocessing
import torch.nn.functional as F
from typing import Tuple

# ----------------------------------------------------
# A. Fixed Point ì–‘ìí™” í•¨ìˆ˜ ì •ì˜
# ----------------------------------------------------
def quantize_qmn(tensor: torch.Tensor, m: int, n: int) -> torch.Tensor:
    """
    Floating Point í…ì„œë¥¼ Qm.n Fixed Point í¬ë§·ìœ¼ë¡œ ì–‘ìí™”í•©ë‹ˆë‹¤.
    1. Scale (2^n ê³±í•˜ê¸°) -> 2. Rounding -> 3. Clip (ë²”ìœ„ ì œí•œ) -> 4. Unscale
    """
    
    scale_factor = 2**n
    
    # Qm.n í¬ë§·ì˜ ìµœëŒ€/ìµœì†Œ ê°’ (mì€ Sign bit í¬í•¨ ì´ ë¹„íŠ¸ ìˆ˜, ì¦‰ m-1ì€ ì •ìˆ˜ ë°ì´í„° ë¹„íŠ¸ ìˆ˜)
    # 16bit Signed Fixed Pointì˜ ì‹¤ì œ í‘œí˜„ ë²”ìœ„: [-2^(m-1), 2^(m-1) - 2^(-n)]
    max_representable_val = 2**(m - 1) - 2**(-n)
    min_representable_val = -2**(m - 1)
    
    # 2. ìŠ¤ì¼€ì¼ ë° ë¼ìš´ë”©
    quantized_tensor = torch.round(tensor * scale_factor)
    
    # 3. í´ë¦¬í•‘ (Overflow/Underflow ë°©ì§€)
    # ì •ìˆ˜ë¶€ ìµœëŒ“ê°’/ìµœì†Ÿê°’ì— ë§ì¶° í´ë¦¬í•‘
    quantized_tensor = torch.clamp(
        quantized_tensor, 
        min=min_representable_val * scale_factor, 
        max=max_representable_val * scale_factor
    )
    
    # 4. ì–¸ìŠ¤ì¼€ì¼ë§
    return quantized_tensor / scale_factor

# ====================================================
# 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ê²½ë¡œ ì„¤ì • (ìµœì í™”)
# ====================================================
# ğŸš¨ğŸš¨ğŸš¨ 8bit ì–‘ìí™”ëœ NPY íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì • (FPGAì™€ ë°ì´í„° ì¼ì¹˜) ğŸš¨ğŸš¨ğŸš¨
clear_command_npy_folder_path = "C:/Users/11e26/Desktop/internship/source/clear_command_trimmed/spike_16bit_regenerated"
neg_command_npy_folder_path = "C:/Users/11e26/Desktop/internship/source/clear_negative_command/spike_16bit_regenerated" 
FPGA_WEIGHTS_DIR = "./fpga_weights/" 

N_MELS = 20 
NUM_HIDDENS_1 = 128  # ğŸš¨ ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ
NUM_HIDDENS_2 = 128  # ğŸš¨ ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ ì¶”ê°€
NUM_OUTPUTS = 2
BETA = 0.95
THRESHOLD = 0.5      
spike_grad = surrogate.atan()

T_MAX = 3000         
BATCH_SIZE = 64      
NUM_EPOCHS = 50      
LEARNING_RATE = 5e-4
# ğŸš¨ QAT Finetuningì„ ìœ„í•œ ì„¤ì •
FINETUNE_EPOCHS = 30       # QATë¡œ ì¶”ê°€ í›ˆë ¨í•  ì—í¬í¬ (50ë³´ë‹¤ ì§§ê²Œ)
FINETUNE_LR = LEARNING_RATE / 10.0  # ğŸš¨ ë” ì‘ì€ LR ì‚¬ìš© (ì˜ˆ: 5e-5)
PTQ_MODEL_PATH = "./wws_snn_final_weights.pth"   # ğŸš¨ ë°©ê¸ˆ í›ˆë ¨í•œ PTQ ëª¨ë¸ ê²½ë¡œ
QAT_MODEL_SAVE_PATH = "./wws_snn_qat_final_weights.pth" # ğŸš¨ QAT ìµœì¢… ëª¨ë¸ ì €ì¥ ê²½ë¡œ
FPGA_QAT_WEIGHTS_DIR = "./fpga_weights_qat/"          # ğŸš¨ QAT FPGA ê°€ì¤‘ì¹˜ ì €ì¥ ê²½ë¡œ

# DataLoader ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
NUM_WORKERS = multiprocessing.cpu_count() - 1
if NUM_WORKERS < 1: NUM_WORKERS = 1

# beta = 0.95
class SpikeDataset(Dataset):
    def __init__(self, file_paths, labels, T_max=T_MAX, n_mels=N_MELS):
        self.file_paths = file_paths
        self.labels = labels
        self.T_max = T_max
        self.n_mels = n_mels

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        file_path = self.file_paths[idx]
        label = self.labels[idx]

        try:
            spike_data_np = np.load(file_path) 
        except Exception as e:
            print(f"Error loading {file_path}: {e}")
            spike_data_np = np.zeros((self.T_max, self.n_mels), dtype=np.float32)

        if spike_data_np.shape[0] > self.T_max:
             spike_data_np = spike_data_np[:self.T_max, :]
        elif spike_data_np.shape[0] < self.T_max:
             padding = np.zeros((self.T_max - spike_data_np.shape[0], self.n_mels), dtype=np.float32)
             spike_data_np = np.vstack([spike_data_np, padding])
        
        data_tensor = torch.as_tensor(spike_data_np, dtype=torch.float32)
        label_tensor = torch.tensor(label, dtype=torch.long)
        
        return data_tensor, label_tensor

# ----------------------------------------------------
# 3. SNN ëª¨ë¸ í´ë˜ìŠ¤ (Fixed Point QAT ë¡œì§ í†µí•©)
# ----------------------------------------------------
class WWS_SNN(nn.Module):
    # ğŸš¨ Fixed Point í¬ë§· ì •ì˜ (ë™ì¼)
    QW_M, QW_N = 7, 9 
    QT_M, QT_N = 5, 11 
    
    # ğŸš¨ is_qat í”Œë˜ê·¸ ì¶”ê°€
    def __init__(self, num_inputs, num_hiddens_1, num_hiddens_2, num_outputs, beta, threshold, spike_grad, is_qat=False):
        super().__init__()
        
        self.is_qat = is_qat
        
        # ğŸš¨ [ì¤‘ìš”] PTQ í›ˆë ¨ ë•Œì²˜ëŸ¼ thresholdëŠ” 'float'ë¡œ ì´ˆê¸°í™”
        self.float_threshold = threshold 
        quantized_threshold = threshold 
        
        if self.is_qat:
            # QAT ëª¨ë“œì¼ ê²½ìš°ì—ë§Œ ì„ê³„ê°’ë„ ì–‘ìí™”
            quantized_threshold = quantize_qmn(torch.tensor(threshold), self.QT_M, self.QT_N).item()
        # 1. ì…ë ¥ì¸µ -> ì€ë‹‰ì¸µ 1
        self.fc1 = nn.Linear(num_inputs, num_hiddens_1)
        self.lif1 = snn.Leaky(beta=beta, threshold=quantized_threshold, spike_grad=spike_grad, reset_mechanism="subtract")
        
        # 2. ğŸš¨ ì€ë‹‰ì¸µ 1 -> ì€ë‹‰ì¸µ 2 (ìƒˆë¡œ ì¶”ê°€)
        self.fc2 = nn.Linear(num_hiddens_1, num_hiddens_2)
        self.lif2 = snn.Leaky(beta=beta, threshold=quantized_threshold, spike_grad=spike_grad, reset_mechanism="subtract")
        
        # 3. ğŸš¨ ì€ë‹‰ì¸µ 2 -> ì¶œë ¥ì¸µ (ì´ë¦„ ë³€ê²½ fc2->fc3, lif2->lif3)
        self.fc3 = nn.Linear(num_hiddens_2, num_outputs)
        self.lif3 = snn.Leaky(beta=beta, threshold=quantized_threshold, spike_grad=spike_grad, reset_mechanism="subtract")
        
        self.init_state()
        
    def enable_qat(self):
        print("âœ… QAT (Quantization-Aware Training) Finetuning ëª¨ë“œë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.")
        self.is_qat = True
        
        # 1. ğŸš¨ [ìˆ˜ì •] .item()ì„ ì œê±°í•˜ì—¬ PyTorch í…ì„œ ìì²´ë¥¼ ìƒì„±
        q_thresh_tensor = quantize_qmn(torch.tensor(self.float_threshold), self.QT_M, self.QT_N)

        # 2. ğŸš¨ [ìˆ˜ì •] ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” device (cpu or cuda)ë¡œ í…ì„œë¥¼ ì´ë™
        #    (self.fc1.weight.deviceê°€ í˜„ì¬ ëª¨ë¸ì´ ìˆëŠ” ì¥ì¹˜ë¥¼ ì•Œë ¤ì¤Œ)
        device = self.fc1.weight.device 
        q_thresh_tensor = q_thresh_tensor.to(device)
        
        # 3. í…ì„œë¥¼ lif ë‰´ëŸ°ì˜ thresholdì— í• ë‹¹ (ì´ì œ Typeì´ ë§ìŒ)
        self.lif1.threshold = q_thresh_tensor
        self.lif2.threshold = q_thresh_tensor
        self.lif3.threshold = q_thresh_tensor

    def init_state(self):
        self.mem1 = self.lif1.init_leaky()
        self.mem2 = self.lif2.init_leaky() # ğŸš¨ mem2 ì¶”ê°€
        self.mem3 = self.lif3.init_leaky() # ğŸš¨ mem3 (ê¸°ì¡´ mem2)

    def quantize_parameters(self):
        # FC1 (Q7.9)
        self.fc1.weight.data = quantize_qmn(self.fc1.weight.data, self.QW_M, self.QW_N)
        self.fc1.bias.data = quantize_qmn(self.fc1.bias.data, self.QW_M, self.QW_N)
        
        # ğŸš¨ FC2 (ìƒˆë¡œ ì¶”ê°€) (Q7.9)
        self.fc2.weight.data = quantize_qmn(self.fc2.weight.data, self.QW_M, self.QW_N)
        self.fc2.bias.data = quantize_qmn(self.fc2.bias.data, self.QW_M, self.QW_N)
        
        # ğŸš¨ FC3 (ì´ë¦„ ë³€ê²½) (Q7.9)
        self.fc3.weight.data = quantize_qmn(self.fc3.weight.data, self.QW_M, self.QW_N)
        self.fc3.bias.data = quantize_qmn(self.fc3.bias.data, self.QW_M, self.QW_N)

    # ğŸš¨ forward í•¨ìˆ˜ ì¸ì ë° ë‚´ë¶€ ë¡œì§ ìˆ˜ì •
    def forward(self, x, mem1, mem2, mem3):
        
        if self.is_qat:
            # === QAT (Fake Quantization) ëª¨ë“œ ===
            
            w1 = self.fc1.weight + (quantize_qmn(self.fc1.weight, self.QW_M, self.QW_N) - self.fc1.weight).detach()
            b1 = self.fc1.bias + (quantize_qmn(self.fc1.bias, self.QW_M, self.QW_N) - self.fc1.bias).detach()
            w2 = self.fc2.weight + (quantize_qmn(self.fc2.weight, self.QW_M, self.QW_N) - self.fc2.weight).detach()
            b2 = self.fc2.bias + (quantize_qmn(self.fc2.bias, self.QW_M, self.QW_N) - self.fc2.bias).detach()
            w3 = self.fc3.weight + (quantize_qmn(self.fc3.weight, self.QW_M, self.QW_N) - self.fc3.weight).detach()
            b3 = self.fc3.bias + (quantize_qmn(self.fc3.bias, self.QW_M, self.QW_N) - self.fc3.bias).detach()
            
            # 2. ì–‘ìí™”ëœ ê°€ì¤‘ì¹˜ë¡œ ì§ì ‘ ì—°ì‚°
            cur1 = F.linear(x, w1, b1)
            spk1, mem1 = self.lif1(cur1, mem1)
            cur2 = F.linear(spk1, w2, b2)
            spk2, mem2 = self.lif2(cur2, mem2)
            cur3 = F.linear(spk2, w3, b3)
            spk3, mem3 = self.lif3(cur3, mem3)
            
        else:
            # === í‘œì¤€ (Float) PTQ í›ˆë ¨ ëª¨ë“œ ===
            cur1 = self.fc1(x)
            spk1, mem1 = self.lif1(cur1, mem1)
            cur2 = self.fc2(spk1)
            spk2, mem2 = self.lif2(cur2, mem2)
            cur3 = self.fc3(spk2)
            spk3, mem3 = self.lif3(cur3, mem3)
            
        return spk3, mem1, mem2, mem3

# ----------------------------------------------------
# 4. Main ì‹¤í–‰ ë¸”ë¡
# ----------------------------------------------------
if __name__=="__main__":
    
    device = torch.device("cpu")
    print(f"Using device: {device}")
    
    # === A. ë°ì´í„° ë¡œë“œ ë° í†µí•© ===
    
    file_paths = []
    labels = [] 

    # 1. Positive (Alexa, Label = 1) ë°ì´í„° ë¡œë“œ 
    pos_files = glob.glob(os.path.join(clear_command_npy_folder_path, "*.npy"))
    file_paths.extend(pos_files)
    labels.extend([1] * len(pos_files))
    print(f"Positive ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ: {len(pos_files)}ê°œ")

    # 2. Negative (Non-Alexa, Label = 0) ë°ì´í„° ë¡œë“œ 
    neg_files = glob.glob(os.path.join(neg_command_npy_folder_path, "*.npy"))
    file_paths.extend(neg_files)
    labels.extend([0] * len(neg_files))
    print(f"Negative ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ: {len(neg_files)}ê°œ") 
    
    if not file_paths:
        print("ì˜¤ë¥˜: í•™ìŠµì— ì‚¬ìš©í•  NPY íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        exit()


    # 3. DataLoader ìƒì„±
    spike_dataset = SpikeDataset(file_paths, labels, T_max=T_MAX, n_mels=N_MELS)
    train_loader = DataLoader(spike_dataset, 
                              batch_size=BATCH_SIZE, 
                              shuffle=True, 
                              drop_last=True,
                              num_workers=NUM_WORKERS) 
    print(f"ì´ í•™ìŠµ ë°ì´í„°ì…‹ í¬ê¸°: {len(file_paths)}ê°œ ìƒ˜í”Œ")
    
    # Loss Weight ê³„ì‚°
    pos_count = len(pos_files)
    neg_count = len(neg_files)
    weight_for_neg = pos_count / neg_count

    # === B. ëª¨ë¸ ë° PTQ ê°€ì¤‘ì¹˜ ë¡œë“œ ===
    
    class_weights = torch.tensor([weight_for_neg, 1.0], dtype=torch.float32).to(device) 

    # ğŸš¨ is_qat=False (ê¸°ë³¸ê°’)ë¡œ ë¨¼ì € ëª¨ë¸ ê°ì²´ ìƒì„±
    net = WWS_SNN(N_MELS, NUM_HIDDENS_1, NUM_HIDDENS_2, NUM_OUTPUTS, BETA, THRESHOLD, spike_grad, is_qat=False).to(device)
    loss_fn = nn.CrossEntropyLoss(weight=class_weights).to(device) 
    
    # ğŸš¨ 1. ì´ì „ PTQ í›ˆë ¨ ê°€ì¤‘ì¹˜ ë¡œë“œ
    try:
        net.load_state_dict(torch.load(PTQ_MODEL_PATH))
        print(f"âœ… ì„±ê³µ: {PTQ_MODEL_PATH}ì—ì„œ í›ˆë ¨ëœ (PTQ) ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ğŸš¨ ê²½ê³ : {PTQ_MODEL_PATH} ë¡œë“œ ì‹¤íŒ¨. ({e})")
        print("ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” PTQ í›ˆë ¨ì´ ì™„ë£Œëœ í›„ì— ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")
        exit()
        
    # ğŸš¨ 2. QAT Finetuningì„ ìœ„í•œ ìƒˆ ì˜µí‹°ë§ˆì´ì € (ë” ë‚®ì€ LR)
    optimizer = torch.optim.Adam(net.parameters(), lr=FINETUNE_LR, betas=(0.9, 0.999))
    
    # ğŸš¨ 3. QAT ëª¨ë“œ í™œì„±í™”!
    # ì´ í•¨ìˆ˜ê°€ self.is_qat = Trueë¡œ ë°”ê¾¸ê³ , thresholdë„ ì–‘ìí™”í•¨
    net.enable_qat() 
    
    print(f"QAT Finetuning ì¤€ë¹„ ì™„ë£Œ. (Epochs: {FINETUNE_EPOCHS}, LR: {FINETUNE_LR})")
    
    # === C. SNN í›ˆë ¨ ë£¨í”„ (QAT ì ìš©) ===

    for epoch in range(FINETUNE_EPOCHS): # ğŸš¨ FINETUNE_EPOCHS ì‚¬ìš©
        net.train()
        total_loss = 0
        total_correct = 0
        
        for inputs, targets in train_loader:
            # ... (inputs, targets .to(device)) ...
            net.init_state() 
            total_output_spikes = torch.zeros(inputs.size(0), NUM_OUTPUTS).to(device) 
            optimizer.zero_grad()
            T_max_current = inputs.size(1) 
            
            # ğŸš¨ [ì¤‘ìš”] net.quantize_parameters()ëŠ” ì ˆëŒ€ í˜¸ì¶œ ê¸ˆì§€!
            # `forward` í•¨ìˆ˜ê°€ ë‚´ë¶€ì ìœ¼ë¡œ STE QATë¥¼ ìˆ˜í–‰í•¨
            
            # 2. ì‹œê°„ ì¶• (T) ì‹œë®¬ë ˆì´ì…˜
            for step in range(T_max_current):
                spk_out, net.mem1, net.mem2, net.mem3 = net(inputs[:, step, :], net.mem1, net.mem2, net.mem3)
                total_output_spikes += spk_out
                
            # 3. ì†ì‹¤ ê³„ì‚° ë° ì—­ì „íŒŒ
            loss = loss_fn(total_output_spikes, targets)
            loss.backward()
            optimizer.step() # ğŸ‘ˆ ì´ ì˜µí‹°ë§ˆì´ì €ëŠ” 32bit ì›ë³¸ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•¨
            
            total_loss += loss.item()
            
            # 4. ì •í™•ë„ ê³„ì‚°
            _, predicted = torch.max(total_output_spikes, 1)
            total_correct += (predicted == targets).sum().item()

        # ì—í¬í¬ ê²°ê³¼ ì¶œë ¥
        avg_loss = total_loss / len(train_loader)
        avg_acc = total_correct / (len(train_loader) * BATCH_SIZE) * 100
        
        if (epoch + 1) % 5 == 0:
            print(f"Epoch {epoch+1:03d} | Loss: {avg_loss:.4f} | Training Accuracy: {avg_acc:.2f}%")

# === D. QAT í•™ìŠµ ê²°ê³¼ ì €ì¥ ë° FPGA ë³€í™˜ìš© ì¶”ì¶œ ===
    
    print(f"\nâœ… QAT Finetuning ì™„ë£Œ.")
    
    # ğŸš¨ ìµœì¢… ì €ì¥ ì „, ê°€ì¤‘ì¹˜ë¥¼ ë‹¤ì‹œ í•œë²ˆ ì–‘ìí™”í•˜ì—¬ FPGAìš©ìœ¼ë¡œ ì €ì¥
    # (QAT í›ˆë ¨ìœ¼ë¡œ ë¯¸ì„¸í•˜ê²Œ ë°”ë€ 32bit ê°€ì¤‘ì¹˜ë¥¼ ìµœì¢… Q7.9ë¡œ ë³€í™˜)
    net.quantize_parameters()
    
    torch.save(net.state_dict(), QAT_MODEL_SAVE_PATH) # ğŸš¨ ìƒˆ ê²½ë¡œì— ì €ì¥
    print(f"\nâœ… QAT Finetuned ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ {QAT_MODEL_SAVE_PATH}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    os.makedirs(FPGA_QAT_WEIGHTS_DIR, exist_ok=True) # ğŸš¨ ìƒˆ ê²½ë¡œì— ì €ì¥
    
    # ğŸš¨ ì €ì¥ ë¡œì§ì€ ë™ì¼ (ê²½ë¡œë§Œ ë³€ê²½)
    W1 = net.fc1.weight.data.numpy()
    B1 = net.fc1.bias.data.numpy()
    np.save(os.path.join(FPGA_QAT_WEIGHTS_DIR, "W1.npy"), W1)
    np.save(os.path.join(FPGA_QAT_WEIGHTS_DIR, "B1.npy"), B1)
    
    W2 = net.fc2.weight.data.numpy()
    B2 = net.fc2.bias.data.numpy()
    np.save(os.path.join(FPGA_QAT_WEIGHTS_DIR, "W2.npy"), W2)
    np.save(os.path.join(FPGA_QAT_WEIGHTS_DIR, "B2.npy"), B2)
    
    W3 = net.fc3.weight.data.numpy()
    B3 = net.fc3.bias.data.numpy()
    np.save(os.path.join(FPGA_QAT_WEIGHTS_DIR, "W3.npy"), W3)
    np.save(os.path.join(FPGA_QAT_WEIGHTS_DIR, "B3.npy"), B3)
    
    # LIF íŒŒë¼ë¯¸í„° ì €ì¥ (QATì´ë¯€ë¡œ ì–‘ìí™”ëœ threshold ì‚¬ìš©)
    lif_params = {
        'BETA_VAL': BETA, 
        'THRESHOLD_VAL': net.lif1.threshold, # ğŸš¨ enable_qat()ì—ì„œ ì´ë¯¸ ì–‘ìí™”ë¨
        'QW_M': WWS_SNN.QW_M,
        'QW_N': WWS_SNN.QW_N,
        'QT_M': WWS_SNN.QT_M,
        'QT_N': WWS_SNN.QT_N,
    }
    np.save(os.path.join(FPGA_QAT_WEIGHTS_DIR, "LIF_params.npy"), lif_params)
    print(f"âœ… QAT Finetuned FPGA ê°€ì¤‘ì¹˜ê°€ {FPGA_QAT_WEIGHTS_DIR}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ì‘ë™ ì™„ë£Œ")